{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc+K5ZYZPLfhi3tma8ql6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottzakrzewski/samples/blob/main/Director_of_SEO_iPullRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI4yRz4RrTNf"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request, jsonify\n",
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Replace with your actual Google Custom Search Engine ID\n",
        "CSE_ID = \"YOUR_CSE_ID\"\n",
        "API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "# Load pre-trained sentiment analysis model (e.g., BERT)\n",
        "model_name = \"bert-base-uncased-finetuned-sst-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load Hugging Face embeddings for semantic search\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load OpenAI LLM (replace with your actual API key)\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", api_key=\"YOUR_OPENAI_API_KEY\")\n",
        "\n",
        "def google_search(search_term):\n",
        "    \"\"\"\n",
        "    Performs a Google Custom Search and returns the results.\n",
        "    \"\"\"\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=API_KEY)\n",
        "    res = service.cse().list(\n",
        "        q=search_term,\n",
        "        cx=CSE_ID,\n",
        "    ).execute()\n",
        "    return res['items']\n",
        "\n",
        "def get_domain_authority(domain):\n",
        "    \"\"\"\n",
        "    Fetches Domain Authority (DA) using an external API (replace with your preferred API).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_key = \"YOUR_EXTERNAL_API_KEY\"  # Replace with your actual API key\n",
        "        url = f\"https://api.example.com/da?domain={domain}&api_key={api_key}\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        return data['da']\n",
        "    except (requests.exceptions.RequestException, KeyError, ValueError) as e:\n",
        "        print(f\"Error fetching DA for {domain}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_page_title(url):\n",
        "    \"\"\"\n",
        "    Fetches page title from the given URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        title = soup.find('title').text.strip()\n",
        "        return title\n",
        "    except (requests.exceptions.RequestException, AttributeError) as e:\n",
        "        print(f\"Error fetching page title for {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_page_content(url):\n",
        "    \"\"\"\n",
        "    Fetches and extracts relevant text content from the given URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract relevant content (e.g., paragraphs, headings)\n",
        "        text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "        return text\n",
        "    except (requests.exceptions.RequestException, AttributeError) as e:\n",
        "        print(f\"Error fetching page content for {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_content_similarity(search_term, page_content):\n",
        "    \"\"\"\n",
        "    Calculates the similarity between the search term and the page content using TF-IDF.\n",
        "    \"\"\"\n",
        "    corpus = [search_term, page_content]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "    return similarity_score\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"\n",
        "    Predicts the sentiment of the given text using a pre-trained sentiment analysis model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_label_idx = torch.argmax(logits, dim=1).item()\n",
        "    labels = [\"negative\", \"positive\"]  # Assuming a binary sentiment classification model\n",
        "    predicted_label = labels[predicted_label_idx]\n",
        "    return predicted_label\n",
        "\n",
        "def create_vectorstore(documents):\n",
        "    \"\"\"\n",
        "    Creates a FAISS vectorstore for efficient semantic search.\n",
        "    \"\"\"\n",
        "    return FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "def get_relevant_documents(vectorstore, query):\n",
        "    \"\"\"\n",
        "    Retrieves relevant documents from the vectorstore based on the query.\n",
        "    \"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return docs\n",
        "\n",
        "def generate_answer(query, relevant_documents):\n",
        "    \"\"\"\n",
        "    Generates an answer to the query using an LLM and relevant documents.\n",
        "    \"\"\"\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    answer = qa_chain.run(query)\n",
        "    return answer\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    if request.method == 'POST':\n",
        "        search_term = request.form['search']\n",
        "        results = google_search(search_term)\n",
        "\n",
        "        # Enhance results with additional data\n",
        "        for result in results:\n",
        "            domain = re.match(r'^(?:http|https)://[^/]+', result['link']).group(0)\n",
        "            result['da'] = get_domain_authority(domain)\n",
        "            result['title'] = get_page_title(result['link'])\n",
        "            result['content'] = get_page_content(result['link'])\n",
        "            result['similarity'] = calculate_content_similarity(search_term, result['content'])\n",
        "            result['sentiment'] = predict_sentiment(result['content'])\n",
        "\n",
        "        # Create a list of documents for vectorstore\n",
        "        documents = [\n",
        "            {\"text\": result['content'], \"metadata\": {\"title\": result['title'], \"url\": result['link']}}\n",
        "            for result in results\n",
        "        ]\n",
        "\n",
        "        # Create vectorstore\n",
        "        vectorstore = create_vectorstore(documents)\n",
        "\n",
        "        # Get relevant documents for the search term\n",
        "        relevant_docs = get_relevant_documents(vectorstore, search_term)\n",
        "\n",
        "        # Generate an answer using LLM and relevant documents\n",
        "        answer = generate_answer(search_term, relevant_docs)\n",
        "\n",
        "        return render_template('results.html', results=results, answer=answer)\n",
        "    return render_template('index.html')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ]
    }
  ]
}